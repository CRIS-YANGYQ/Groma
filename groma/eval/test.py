# import os
# import copy
import torch
# import argparse
# import requests
# from io import BytesIO
# from PIL import Image, ImageDraw
# from transformers.image_transforms import center_to_corners_format
# from transformers import AutoTokenizer, AutoImageProcessor, BitsAndBytesConfig
# # import sys
# # sys.path.append('/comp_robot/yangyuqin/workplace/Multi-model/models/Groma')
# print(1)
# from groma.utils import disable_torch_init
# print(2)
# from groma.model.groma import GromaModel
# print(3)
# from groma.constants import DEFAULT_TOKENS
# print(4)
# from groma.data.conversation import conv_templates
# print(5)


if __name__ == '__main__':
    print("TEST")
    torch.cuda.is_available()